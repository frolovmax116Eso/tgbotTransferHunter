import asyncio
import json
import re
from collections import Counter
from datetime import datetime, timedelta
import sys
import os

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from telethon import TelegramClient
from telethon.sessions import StringSession
from src.utils.database import get_session, UserSession
from src.config import TELEGRAM_API_ID, TELEGRAM_API_HASH, KNOWN_CITIES, CITY_ALIASES
from src.utils.geo import get_coordinates, KNOWN_COORDINATES

GROUP_ID = -1002290679743
MESSAGES_LIMIT = 500

async def analyze_group():
    print(f"=" * 60)
    print(f"–ê–Ω–∞–ª–∏–∑ –≥—Ä—É–ø–ø—ã {GROUP_ID}")
    print(f"=" * 60)
    
    db_session = get_session()
    user_sessions = db_session.query(UserSession).filter(
        UserSession.is_authorized == True,
        UserSession.session_string.isnot(None)
    ).all()
    
    if not user_sessions:
        print("–ù–µ—Ç –∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö —Å–µ—Å—Å–∏–π. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ /auth –≤ –±–æ—Ç–µ.")
        return
    
    user_session = None
    for us in user_sessions:
        print(f"–ü—Ä–æ–±—É–µ–º —Å–µ—Å—Å–∏—é –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {us.user_id}...")
        client = TelegramClient(
            StringSession(us.session_string),
            TELEGRAM_API_ID,
            TELEGRAM_API_HASH
        )
        await client.connect()
        
        if not await client.is_user_authorized():
            print(f"  –°–µ—Å—Å–∏—è {us.user_id} –Ω–µ –∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–∞")
            await client.disconnect()
            continue
        
        try:
            entity = await client.get_entity(GROUP_ID)
            print(f"  ‚úÖ –°–µ—Å—Å–∏—è {us.user_id} –∏–º–µ–µ—Ç –¥–æ—Å—Ç—É–ø –∫ –≥—Ä—É–ø–ø–µ: {entity.title}")
            user_session = us
            break
        except Exception as e:
            print(f"  ‚ùå –ù–µ—Ç –¥–æ—Å—Ç—É–ø–∞: {e}")
            await client.disconnect()
            continue
    
    if not user_session:
        print("\n–ù–∏ –æ–¥–Ω–∞ —Å–µ—Å—Å–∏—è –Ω–µ –∏–º–µ–µ—Ç –¥–æ—Å—Ç—É–ø–∞ –∫ –≥—Ä—É–ø–ø–µ.")
        print("–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–æ—Å—Ç–æ–∏—Ç –≤ —ç—Ç–æ–π –≥—Ä—É–ø–ø–µ.")
        return
    
    print(f"\n–°–æ–±–∏—Ä–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ {MESSAGES_LIMIT} —Å–æ–æ–±—â–µ–Ω–∏–π...")
    
    messages = []
    async for message in client.iter_messages(entity, limit=MESSAGES_LIMIT):
        if message.text:
            messages.append({
                'id': message.id,
                'date': message.date.isoformat(),
                'text': message.text,
                'sender_id': message.sender_id
            })
    
    print(f"–ü–æ–ª—É—á–µ–Ω–æ {len(messages)} —Å–æ–æ–±—â–µ–Ω–∏–π —Å —Ç–µ–∫—Å—Ç–æ–º")
    
    cities_found = Counter()
    unknown_locations = Counter()
    price_patterns = Counter()
    order_examples = []
    keywords_found = Counter()
    
    known_cities_lower = {c.lower(): c for c in KNOWN_CITIES}
    aliases_lower = {a.lower(): v for a, v in CITY_ALIASES.items()}
    known_coords_lower = set(KNOWN_COORDINATES.keys())
    
    route_patterns = [
        r'([–ê-–Ø–∞-—è–Å—ë][–ê-–Ø–∞-—è–Å—ë\s\-]+?)\s*[-‚Äì‚Äî‚Üí>]+\s*([–ê-–Ø–∞-—è–Å—ë][–ê-–Ø–∞-—è–Å—ë\s\-]+)',
        r'(?:–æ—Ç–∫—É–¥–∞|–∏–∑|–æ—Ç|—Å)[:\s]*([–ê-–Ø–∞-—è–Å—ë][–ê-–Ø–∞-—è–Å—ë\s\-]+?)[\s,\-‚Äì‚Äî]+(?:–∫—É–¥–∞|–≤|–¥–æ|–Ω–∞)[:\s]*([–ê-–Ø–∞-—è–Å—ë][–ê-–Ø–∞-—è–Å—ë\s\-]+)',
        r'(?:–ê|–∞)[:\s]*([–ê-–Ø–∞-—è–Å—ë][–ê-–Ø–∞-—è–Å—ë\s\-]+?)[\s,\-‚Äì‚Äî]+(?:–ë|–±)[:\s]*([–ê-–Ø–∞-—è–Å—ë][–ê-–Ø–∞-—è–Å—ë\s\-]+)',
    ]
    
    price_regex = [
        (r'(\d{1,3})[,](\d{3})\s*(?:—Ä—É–±|‚ÇΩ|—Ä\.?)?', 'comma'),
        (r'(\d{3,5})\s*(?:—Ä—É–±|‚ÇΩ|—Ä\.?\b)', 'direct'),
        (r'(\d{1,2})\s*(?:–∫|—Ç—ã—Å|—Ç)\.?', 'thousands'),
    ]
    
    order_keywords = ['–∑–∞–∫–∞–∑', '–ø–∞—Å—Å–∞–∂–∏—Ä', '—á–µ–ª', '—á–µ–ª–æ–≤–µ–∫', '–ø–æ–µ–∑–¥–∫–∞', '—Ç—Ä–∞–Ω—Å—Ñ–µ—Ä', 
                      '–º–µ–∂–≥–æ—Ä–æ–¥', '—Ç–∞–∫—Å–∏', '–≤–æ–¥–∏—Ç–µ–ª—å', '–º–∏–Ω–∏–≤–µ–Ω', '—Å–µ–¥–∞–Ω', '–∫–æ–º—Ñ–æ—Ä—Ç',
                      '–±–∏–∑–Ω–µ—Å', '—ç–∫–æ–Ω–æ–º', '–±–∞–≥–∞–∂', '—á–µ–º–æ–¥–∞–Ω', '–¥–µ—Ç—Å–∫–æ–µ –∫—Ä–µ—Å–ª–æ']
    
    for msg in messages:
        text = msg['text']
        text_lower = text.lower()
        
        for kw in order_keywords:
            if kw in text_lower:
                keywords_found[kw] += 1
        
        for pattern in route_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                point_a = match[0].strip()
                point_b = match[1].strip()
                
                for point in [point_a, point_b]:
                    point_clean = re.sub(r'[^\w\s\-]', '', point).strip()
                    point_lower = point_clean.lower()
                    
                    if len(point_clean) < 3:
                        continue
                    
                    if point_lower in known_cities_lower:
                        cities_found[known_cities_lower[point_lower]] += 1
                    elif point_lower in aliases_lower:
                        cities_found[aliases_lower[point_lower]] += 1
                    elif point_lower in known_coords_lower:
                        cities_found[point_clean] += 1
                    else:
                        if len(point_clean) >= 4 and point_clean[0].isupper():
                            unknown_locations[point_clean] += 1
                
                if point_a and point_b and len(order_examples) < 50:
                    order_examples.append({
                        'text': text[:300],
                        'point_a': point_a,
                        'point_b': point_b
                    })
        
        for regex, pattern_type in price_regex:
            if re.search(regex, text, re.IGNORECASE):
                price_patterns[pattern_type] += 1
    
    print("\n" + "=" * 60)
    print("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –ê–ù–ê–õ–ò–ó–ê")
    print("=" * 60)
    
    print(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print(f"   –í—Å–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏–π: {len(messages)}")
    print(f"   –ù–∞–π–¥–µ–Ω–æ –º–∞—Ä—à—Ä—É—Ç–æ–≤: {sum(cities_found.values()) // 2}")
    
    print(f"\nüèôÔ∏è –ò–ó–í–ï–°–¢–ù–´–ï –ì–û–†–û–î–ê (—Ç–æ–ø-20):")
    for city, count in cities_found.most_common(20):
        print(f"   {city}: {count}")
    
    print(f"\n‚ùì –ù–ï–ò–ó–í–ï–°–¢–ù–´–ï –õ–û–ö–ê–¶–ò–ò (—Ç–æ–ø-30):")
    for loc, count in unknown_locations.most_common(30):
        coords = get_coordinates(loc)
        status = "‚úÖ" if coords else "‚ùå"
        print(f"   {status} {loc}: {count} (coords: {coords})")
    
    print(f"\nüí∞ –ü–ê–¢–¢–ï–†–ù–´ –¶–ï–ù:")
    for pattern, count in price_patterns.most_common():
        print(f"   {pattern}: {count}")
    
    print(f"\nüîë –ö–õ–Æ–ß–ï–í–´–ï –°–õ–û–í–ê:")
    for kw, count in keywords_found.most_common():
        print(f"   {kw}: {count}")
    
    results = {
        'group_id': GROUP_ID,
        'analyzed_at': datetime.now().isoformat(),
        'total_messages': len(messages),
        'known_cities': dict(cities_found.most_common(50)),
        'unknown_locations': dict(unknown_locations.most_common(50)),
        'price_patterns': dict(price_patterns),
        'keywords': dict(keywords_found),
        'order_examples': order_examples[:20]
    }
    
    with open('scripts/group_analysis.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ scripts/group_analysis.json")
    
    if unknown_locations:
        print(f"\nüìù –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –î–û–ë–ê–í–õ–ï–ù–ò–Æ:")
        needs_coords = []
        for loc, count in unknown_locations.most_common(20):
            if count >= 2:
                coords = get_coordinates(loc)
                if coords:
                    needs_coords.append(f"    '{loc.lower()}': {coords},")
        
        if needs_coords:
            print("\n–î–æ–±–∞–≤–∏—Ç—å –≤ KNOWN_COORDINATES (src/utils/geo.py):")
            for line in needs_coords:
                print(line)
    
    await client.disconnect()
    db_session.close()

if __name__ == "__main__":
    asyncio.run(analyze_group())
